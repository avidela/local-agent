# Database Migrations Implementation Guide

## 🎯 Overview

This document provides the **exact implementation** for database migrations using Alembic, replacing the inconsistent `Base.metadata.create_all` approach shown in other documentation.

## 🔧 Correct Database Initialization Pattern

### Replace Current `init_db()` Implementation

**❌ WRONG (Current Documentation Shows This)**:
```python
# DON'T USE THIS PATTERN - Doesn't run seed migrations
async def init_db():
    from ..models.base import BaseModel
    async with engine.begin() as conn:
        from ..models import *
        await conn.run_sync(BaseModel.metadata.create_all)
```

**✅ CORRECT (Use This Instead)**:
```python
# src/database/base.py - CORRECT Alembic-based implementation
async def init_db() -> None:
    """Initialize database using Alembic migrations"""
    from alembic.config import Config
    from alembic import command
    import asyncio
    import os
    
    def run_migrations():
        """Run Alembic migrations in sync context"""
        alembic_cfg = Config("alembic.ini")
        command.upgrade(alembic_cfg, "head")
    
    # Run migrations in executor to avoid blocking
    loop = asyncio.get_event_loop()
    await loop.run_in_executor(None, run_migrations)
    
    print("Database migrations completed successfully")
```

## 📁 Required Files Structure

### 1. Initial Migration File
**File**: `migrations/versions/001_initial_migration.py`
```python
"""Initial migration

Revision ID: 001
Revises: 
Create Date: 2024-01-01 12:00:00.000000

"""
from typing import Sequence, Union
from alembic import op
import sqlalchemy as sa
from sqlalchemy.dialects import postgresql

# revision identifiers
revision: str = '001'
down_revision: Union[str, None] = None
branch_labels: Union[str, Sequence[str], None] = None
depends_on: Union[str, Sequence[str], None] = None

def upgrade() -> None:
    # ### commands auto generated by Alembic - will be replaced by actual migration ###
    # Create users table
    op.create_table('users',
    sa.Column('id', sa.Integer(), nullable=False),
    sa.Column('username', sa.String(50), nullable=False),
    sa.Column('email', sa.String(255), nullable=False),
    sa.Column('password_hash', sa.String(255), nullable=False),
    sa.Column('full_name', sa.String(200), nullable=True),
    sa.Column('role', sa.Enum('user', 'admin', 'moderator', name='userrole'), nullable=False),
    sa.Column('is_active', sa.Boolean(), nullable=False),
    sa.Column('created_at', sa.DateTime(timezone=True), server_default=sa.text('now()'), nullable=False),
    sa.Column('updated_at', sa.DateTime(timezone=True), server_default=sa.text('now()'), nullable=False),
    sa.PrimaryKeyConstraint('id')
    )
    op.create_index(op.f('ix_users_email'), 'users', ['email'], unique=True)
    op.create_index(op.f('ix_users_username'), 'users', ['username'], unique=True)
    
    # Create agents table
    op.create_table('agents',
    sa.Column('id', sa.Integer(), nullable=False),
    sa.Column('name', sa.String(100), nullable=False),
    sa.Column('description', sa.Text(), nullable=True),
    sa.Column('system_prompt', sa.Text(), nullable=False),
    sa.Column('model_provider', sa.Enum('openai', 'anthropic', 'google', name='modelprovider'), nullable=False),
    sa.Column('model_name', sa.String(100), nullable=False),
    sa.Column('temperature', sa.Float(), nullable=False),
    sa.Column('max_tokens', sa.Integer(), nullable=True),
    sa.Column('tools', postgresql.JSONB(astext_type=sa.Text()), nullable=False),
    sa.Column('output_type', sa.String(100), nullable=True),
    sa.Column('retries', sa.Integer(), nullable=False),
    sa.Column('is_public', sa.Boolean(), nullable=False),
    sa.Column('is_active', sa.Boolean(), nullable=False),
    sa.Column('usage_count', sa.Integer(), nullable=False),
    sa.Column('owner_id', sa.Integer(), nullable=False),
    sa.Column('created_at', sa.DateTime(timezone=True), server_default=sa.text('now()'), nullable=False),
    sa.Column('updated_at', sa.DateTime(timezone=True), server_default=sa.text('now()'), nullable=False),
    sa.ForeignKeyConstraint(['owner_id'], ['users.id'], ),
    sa.PrimaryKeyConstraint('id')
    )
    op.create_index(op.f('ix_agents_name'), 'agents', ['name'], unique=False)
    
    # Create sessions table
    op.create_table('sessions',
    sa.Column('id', sa.Integer(), nullable=False),
    sa.Column('session_id', sa.String(36), nullable=False),
    sa.Column('title', sa.String(200), nullable=True),
    sa.Column('status', sa.Enum('active', 'completed', 'failed', 'archived', name='sessionstatus'), nullable=False),
    sa.Column('total_cost', sa.Float(), nullable=False),
    sa.Column('total_tokens', sa.Integer(), nullable=False),
    sa.Column('request_tokens', sa.Integer(), nullable=False),
    sa.Column('response_tokens', sa.Integer(), nullable=False),
    sa.Column('metadata', postgresql.JSONB(astext_type=sa.Text()), nullable=False),
    sa.Column('user_id', sa.Integer(), nullable=False),
    sa.Column('agent_id', sa.Integer(), nullable=False),
    sa.Column('created_at', sa.DateTime(timezone=True), server_default=sa.text('now()'), nullable=False),
    sa.Column('updated_at', sa.DateTime(timezone=True), server_default=sa.text('now()'), nullable=False),
    sa.ForeignKeyConstraint(['agent_id'], ['agents.id'], ),
    sa.ForeignKeyConstraint(['user_id'], ['users.id'], ),
    sa.PrimaryKeyConstraint('id')
    )
    op.create_index(op.f('ix_sessions_session_id'), 'sessions', ['session_id'], unique=True)
    
    # Create messages table
    op.create_table('messages',
    sa.Column('id', sa.Integer(), nullable=False),
    sa.Column('content', postgresql.JSONB(astext_type=sa.Text()), nullable=False),
    sa.Column('role', sa.Enum('system', 'user', 'assistant', 'tool', name='messagerole'), nullable=False),
    sa.Column('attachments', postgresql.ARRAY(sa.String()), nullable=False),
    sa.Column('cost', sa.Float(), nullable=False),
    sa.Column('tokens', sa.Integer(), nullable=False),
    sa.Column('tool_calls', postgresql.JSONB(astext_type=sa.Text()), nullable=True),
    sa.Column('tool_response', postgresql.JSONB(astext_type=sa.Text()), nullable=True),
    sa.Column('metadata', postgresql.JSONB(astext_type=sa.Text()), nullable=False),
    sa.Column('session_id', sa.Integer(), nullable=False),
    sa.Column('created_at', sa.DateTime(timezone=True), server_default=sa.text('now()'), nullable=False),
    sa.Column('updated_at', sa.DateTime(timezone=True), server_default=sa.text('now()'), nullable=False),
    sa.ForeignKeyConstraint(['session_id'], ['sessions.id'], ),
    sa.PrimaryKeyConstraint('id')
    )
    
    # Add additional tables as needed (files, evaluations, workflows, etc.)

def downgrade() -> None:
    # ### commands auto generated by Alembic ###
    op.drop_table('messages')
    op.drop_table('sessions')
    op.drop_table('agents')
    op.drop_table('users')
    # ### end Alembic commands ###
```

### 2. Updated Alembic env.py
**File**: `migrations/env.py`
```python
"""Alembic environment configuration for PydanticAI backend."""

import asyncio
from logging.config import fileConfig
from sqlalchemy import pool
from sqlalchemy.engine import Connection
from sqlalchemy.ext.asyncio import async_engine_from_config

from alembic import context

# this is the Alembic Config object
config = context.config

# Interpret the config file for Python logging
if config.config_file_name is not None:
    fileConfig(config.config_file_name)

# Import your models here for autogenerate to work
import sys
import os
sys.path.append(os.path.dirname(os.path.dirname(os.path.abspath(__file__))))

# Import all models to ensure they're registered
from src.database.models import Base
target_metadata = Base.metadata

def run_migrations_offline() -> None:
    """Run migrations in 'offline' mode."""
    url = config.get_main_option("sqlalchemy.url")
    context.configure(
        url=url,
        target_metadata=target_metadata,
        literal_binds=True,
        dialect_opts={"paramstyle": "named"},
    )

    with context.begin_transaction():
        context.run_migrations()

def do_run_migrations(connection: Connection) -> None:
    context.configure(connection=connection, target_metadata=target_metadata)

    with context.begin_transaction():
        context.run_migrations()

async def run_async_migrations() -> None:
    """Run migrations in async mode."""
    connectable = async_engine_from_config(
        config.get_section(config.config_ini_section, {}),
        prefix="sqlalchemy.",
        poolclass=pool.NullPool,
    )

    async with connectable.connect() as connection:
        await connection.run_sync(do_run_migrations)

    await connectable.dispose()

def run_migrations_online() -> None:
    """Run migrations in 'online' mode."""
    asyncio.run(run_async_migrations())

if context.is_offline_mode():
    run_migrations_offline()
else:
    run_migrations_online()
```

## 🐳 Docker Integration

### Updated Dockerfile
```dockerfile
# Multi-stage Dockerfile for PydanticAI Agents Service
FROM python:3.11-slim as base

# Set environment variables
ENV PYTHONUNBUFFERED=1 \
    PYTHONDONTWRITEBYTECODE=1 \
    PIP_NO_CACHE_DIR=1 \
    PIP_DISABLE_PIP_VERSION_CHECK=1

# Install system dependencies
RUN apt-get update && apt-get install -y \
    build-essential \
    curl \
    && rm -rf /var/lib/apt/lists/*

# Install uv
RUN pip install uv

# Development stage
FROM base as development

WORKDIR /app

# Copy dependency files
COPY pyproject.toml ./

# Install dependencies
RUN uv sync --dev

# Copy source code
COPY . .

# Create migration entrypoint script
RUN echo '#!/bin/bash\nset -e\necho "Running database migrations..."\nuv run alembic upgrade head\necho "Starting application..."\nexec "$@"' > /entrypoint.sh
RUN chmod +x /entrypoint.sh

# Expose port
EXPOSE 8000

# Use entrypoint that runs migrations first
ENTRYPOINT ["/entrypoint.sh"]
CMD ["uv", "run", "uvicorn", "src.main:app", "--host", "0.0.0.0", "--port", "8000", "--reload"]

# Production stage
FROM base as production

WORKDIR /app

# Copy dependency files
COPY pyproject.toml ./

# Install dependencies (production only)
RUN uv sync --no-dev

# Copy source code
COPY . .

# Create migration entrypoint script
RUN echo '#!/bin/bash\nset -e\necho "Running database migrations..."\nuv run alembic upgrade head\necho "Starting application..."\nexec "$@"' > /entrypoint.sh
RUN chmod +x /entrypoint.sh

# Create non-root user
RUN useradd --create-home --shell /bin/bash app && chown -R app:app /app
USER app

# Expose port
EXPOSE 8000

# Health check
HEALTHCHECK --interval=30s --timeout=30s --start-period=5s --retries=3 \
    CMD curl -f http://localhost:8000/health || exit 1

# Use entrypoint that runs migrations first
ENTRYPOINT ["/entrypoint.sh"]
CMD ["uv", "run", "uvicorn", "src.main:app", "--host", "0.0.0.0", "--port", "8000", "--workers", "4"]
```

## 🛠️ Development Commands

### Generate Initial Migration
```bash
# Run from apps/backends/pydantic_ai directory
uv run alembic revision --autogenerate -m "Initial migration"
```

### Run Migrations
```bash
# Upgrade to latest
uv run alembic upgrade head

# Downgrade one version
uv run alembic downgrade -1

# Show current version
uv run alembic current

# Show migration history
uv run alembic history
```

### Reset Database (Development)
```bash
# Drop and recreate database
docker compose down -v
docker compose up -d db
uv run alembic upgrade head
```

## ✅ Benefits of This Approach

1. **Fixes Volume Deletion Issue**: `docker compose down -v && docker compose up` works perfectly
2. **Runs Seed Data**: `002_seed_demo_user.py` executes automatically
3. **Version Control**: Proper database versioning and rollback support
4. **Production Ready**: Consistent behavior across environments
5. **Development Friendly**: Easy reset and migration commands

## 🎯 Implementation Checklist

- [ ] Replace `init_db()` in `src/database/base.py` with Alembic version
- [ ] Generate `001_initial_migration.py` using `alembic revision --autogenerate`
- [ ] Update Dockerfile with migration entrypoint
- [ ] Test clean database start: `docker compose down -v && docker compose up`
- [ ] Verify all tables created and demo user exists
- [ ] Update other documentation to reference this approach

This implementation ensures reliable, versioned database management that works consistently across development and production environments.